{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 - Intro to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is an open-source cluster computing engine for data processing that is fast and easy to use. It's a unified engine packaged for SQL queries, streaming data, machine learning, and graph processing. Spark supports programming languages such as Python, Java, Scala, and R, and runs in laptops until clusters.\n",
    "\n",
    "To understand what Spark does, imagine a single computer that is useful in watching movies, navigation, or spreadsheet software. This only computer is not powerful enough to perform computations in a massive amount of data. An option is to use a cluster. A cluster is a group of machines that shares resources to work together as a single computer. To make it happens, Spark helps managing and coordinating the group of computers.\n",
    "\n",
    "In the cluster, there is one computer to be the *master* that works splitting up the data and processing tasks. After that, sending to the nodes and aggregating each result. The other computers, called nodes, receive and perform the processing task, realize, and return the result to the Master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading PySpark - the Python API for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\guilherme ferreira\\anaconda3\\lib\\site-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in c:\\users\\guilherme ferreira\\anaconda3\\lib\\site-packages (from pyspark) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SparkContext```: Main entry point for Spark functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=First App>\n",
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# create a spark context\n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    "print(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is a collection of data organized in rows and named columns. Spark DataFrames are built on top of a low level object called Resilient Distributed Datase (RDD). RDD is the core data structured of Spark. The Spark DataFrame is easy to understand and more optimized for complicated operations than RDD.\n",
    "\n",
    "Using Spark DataFrames you are able to query data in your Spark cluster.\n",
    "\n",
    "To start working with DataFrames is necessary to create a ```SparkSession```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001B7F7120348>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a spark session\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view all tables/views in your cluster\n",
    "\n",
    "Creating a table from a file with ```.read```. There is a lot of data sources included in this attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='address', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a csv file\n",
    "df = spark_session.read.csv(\"sample_data/101-intro-to-spark.csv\")\n",
    "\n",
    "# register in the catalog\n",
    "df.registerTempTable(\"address\")\n",
    "\n",
    "# list tables in catalog\n",
    "spark_session.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To query in your table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the command ```<your_spark_session>.sql(query)``` it's possible to query as SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------------+---------+---+------+\n",
      "| _c0|     _c1|              _c2|      _c3|_c4|   _c5|\n",
      "+----+--------+-----------------+---------+---+------+\n",
      "|John|     Doe|120 jefferson st.|Riverside| NJ| 08075|\n",
      "|Jack|McGinnis|     220 hobo Av.|    Phila| PA| 09119|\n",
      "+----+--------+-----------------+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"FROM address SELECT * LIMIT 2\"\n",
    "\n",
    "address2 = spark_session.sql(query) \n",
    "\n",
    "address2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a Spark DataFrame in Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>120 jefferson st.</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>NJ</td>\n",
       "      <td>08075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Jack</td>\n",
       "      <td>McGinnis</td>\n",
       "      <td>220 hobo Av.</td>\n",
       "      <td>Phila</td>\n",
       "      <td>PA</td>\n",
       "      <td>09119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"John \"\"Da Man\"\"\"</td>\n",
       "      <td>Repici</td>\n",
       "      <td>120 Jefferson St.</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>NJ</td>\n",
       "      <td>08075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>\"7452 Terrace \"\"At the Plaza\"\" road\"</td>\n",
       "      <td>SomeTown</td>\n",
       "      <td>SD</td>\n",
       "      <td>91234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>Blankman</td>\n",
       "      <td>None</td>\n",
       "      <td>SomeTown</td>\n",
       "      <td>SD</td>\n",
       "      <td>00298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   surname                                street  \\\n",
       "0               John       Doe                     120 jefferson st.   \n",
       "1               Jack  McGinnis                          220 hobo Av.   \n",
       "2  \"John \"\"Da Man\"\"\"    Repici                     120 Jefferson St.   \n",
       "3            Stephen     Tyler  \"7452 Terrace \"\"At the Plaza\"\" road\"   \n",
       "4               None  Blankman                                  None   \n",
       "\n",
       "        city state zipcode  \n",
       "0  Riverside    NJ   08075  \n",
       "1      Phila    PA   09119  \n",
       "2  Riverside    NJ   08075  \n",
       "3   SomeTown    SD   91234  \n",
       "4   SomeTown    SD   00298  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT * FROM address\"\n",
    "\n",
    "# run the query\n",
    "address = spark_session.sql(query)\n",
    "\n",
    "# convert to pandas dataframe\n",
    "df_address = address.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "columns = [\"name\", \"surname\", \"street\", \"city\", \"state\", \"zipcode\"]\n",
    "df_address.columns = columns\n",
    "df_address.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First:  [Table(name='address', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "Second:  [Table(name='address', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='table_from_pandas', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a pandas dataframe\n",
    "pd_df = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# create spark_df from pd_df\n",
    "spark_df = spark_session.createDataFrame(pd_df)\n",
    "\n",
    "# list tables in catalog\n",
    "print(\"First: \", spark_session.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_df.createTempView(\"table_from_pandas\")\n",
    "\n",
    "# list tables in catalog\n",
    "print(\"Second: \", spark_session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.createTempView()``` register the DataFrame as a table in the catalog.\n",
    "\n",
    "```.createDataFrame()``` create a Spark DataFrame from pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD x DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use RDD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - For a specific reason\n",
    "    - When you need fine-grained control over the physical distribution of data (custom partitioning of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient Distributed Datasets (RDDs) is a collection of immutable Java, Scala or Python objects. \n",
    "    \n",
    "    - In RDD, you are able to store what you want in any format.\n",
    "    - There isn't the concept of row in RDD\n",
    "    - Manipulations or interactions are defined by hand, it will be a lot of manual work.\n",
    "    - operates in parallel\n",
    "\n",
    "The use of RDD can be significantly slowly in Python\n",
    "\n",
    "DataFrames are also immutable collections of data organized by named columns. The use of DataFrames helps you in:\n",
    "\n",
    "    - processing large data sets\n",
    "    - formalize the structure of the data\n",
    "    - looks like a relational database\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc is defined above\n",
    "\n",
    "myCollection = \"A ship in the harbor is safe, but that is not what a ship is for.\\\n",
    "A stitch in time saves nine.\\\n",
    "As you sow, so you shall reap.\\\n",
    "Be slow in choosing, but slower in changing.\\\n",
    "Curiosity killed the cat.\\\n",
    "Don't cast pearls before swine.\\\n",
    "Don't count your chickens before they hatch.\\\n",
    "Don't cross a bridge until you come to it.\\\n",
    "Don't judge a book by its cover.\\\n",
    "Don't put the cart before the horse.\\\n",
    "Early bird catches the worm.\".split(\".\") # list of phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a RDD you ```.parallelize()```  a collection (list or an array of some elements):\n",
    "\n",
    "It is possible creating a RDD from a file using ```.textFile(<file_path>)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = sc.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.collect()``` will run an action to bring it back to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A ship in the harbor is safe, but that is not what a ship is for',\n",
       " 'A stitch in time saves nine',\n",
       " 'As you sow, so you shall reap',\n",
       " 'Be slow in choosing, but slower in changing',\n",
       " 'Curiosity killed the cat',\n",
       " \"Don't cast pearls before swine\",\n",
       " \"Don't count your chickens before they hatch\",\n",
       " \"Don't cross a bridge until you come to it\",\n",
       " \"Don't judge a book by its cover\",\n",
       " \"Don't put the cart before the horse\",\n",
       " 'Early bird catches the worm',\n",
       " '']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A ship in the harbor is safe, but that is not what a ship is for', True),\n",
       " ('A stitch in time saves nine', False),\n",
       " ('As you sow, so you shall reap', False),\n",
       " ('Be slow in choosing, but slower in changing', False),\n",
       " ('Curiosity killed the cat', False)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping the sentences that contains ' a '\n",
    "match_str = \" a \"\n",
    "phrases2 = phrases.map(lambda p: (p, match_str in p))\n",
    "\n",
    "# filtering the map response where the map answer is True\n",
    "phrases2.filter(lambda record: record).take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
