{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic informations II\n",
    "\n",
    "## Reshaping RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# create a spark context\n",
    "sc = SparkContext(\"local\", \"Basic informations II\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 3),(\"b\", 6),(\"a\", 6),(\"c\", 9),(\"a\", 12),(\"c\", 15)])\n",
    "rdd1 = sc.parallelize([4, 5, 7, 8, 9, 35, 4, 23, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.reduceByKey()``` - merge the values from each key from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 216), ('b', 6), ('c', 135)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x, y: x*y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.reduce()``` - merge RDD values and return one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.groupByKey()``` - group the values in each equivalent key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [3, 6, 12]), ('b', [6]), ('c', [9, 15])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.groupBy()``` - return a RDD grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [4, 8, 4, 6, 8]), (1, [5, 7, 9, 35, 23, 5, 7])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.groupBy(lambda x: x % 2).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.aggregate()``` aggregates the values of all partitions\n",
    "\n",
    "arguments:\n",
    "- ```zeroValue``` Initial value tu be used\n",
    "- ```seqOp``` accumulate the results of each partition\n",
    "- ```combOp``` combine the results of all partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroValue = (0,0)\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "rdd1.aggregate(zeroValue, seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.aggregateByKey()``` aggregates the values of each RDD key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (21, 3)), ('b', (6, 1)), ('c', (24, 2))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.aggregateByKey(zeroValue, seqOp, combOp).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.fold()``` aggregates the elements of each partition and answer based in the function passed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.fold(0, lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54528768000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.fold(1, lambda x,y: x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.foldByKey()``` - merge the values of each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 21), ('b', 6), ('c', 24)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.foldByKey(0, lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.keyBy()```- create tuples of RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(0,10),2).keyBy(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (2, 1),\n",
       " (4, 2),\n",
       " (6, 3),\n",
       " (8, 4),\n",
       " (10, 5),\n",
       " (12, 6),\n",
       " (14, 7),\n",
       " (16, 8),\n",
       " (18, 9)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(0,10),2).keyBy(lambda x: x+x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some maths operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.subtract(p)``` - return each value in **self** that is not contained in **p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd2:  [13, 5, 18]\n",
      "rdd3:  [20, 11, 18]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([13, 5, 18])\n",
    "rdd3 = sc.parallelize([20, 11, 18])\n",
    "print(\"rdd2: \", rdd2.collect())\n",
    "print(\"rdd3: \", rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.subtract(rdd3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 11]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.subtractByKey(p)``` return each key in **self** that is not contained in **p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd:  [('a', 3), ('b', 6), ('a', 6), ('c', 9), ('a', 12), ('c', 15)]\n",
      "rdd4:  [('g', 3), ('b', 6), ('e', 6), ('f', 9), ('a', 12), ('d', 18)]\n"
     ]
    }
   ],
   "source": [
    "rdd4 = sc.parallelize([(\"g\", 3),(\"b\", 6),(\"e\", 6),(\"f\", 9),(\"a\", 12),(\"d\", 18)])\n",
    "print(\"rdd: \", rdd.collect())\n",
    "print(\"rdd4: \", rdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 9), ('c', 15)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtractByKey(rdd4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 3), ('d', 18), ('e', 6), ('f', 9)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.subtractByKey(rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.cartesian()``` - return Cartesian product of one RDD and another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 20),\n",
       " (13, 11),\n",
       " (13, 18),\n",
       " (5, 20),\n",
       " (5, 11),\n",
       " (5, 18),\n",
       " (18, 20),\n",
       " (18, 11),\n",
       " (18, 18)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cartesian(rdd3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "```.sortBy(f)``` - sort the RDD by the given function ```f(x)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('a', 6), ('a', 12), ('b', 6), ('c', 9), ('c', 15)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 6), ('a', 6), ('c', 9), ('a', 12), ('c', 15)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.sortByKey()``` - sort the RDD by keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 12), ('b', 6), ('d', 18), ('e', 6), ('f', 9), ('g', 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing and decreasing the number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to increase or decrease the number of partitions in your RDD, consider:\n",
    "\n",
    "  - ```.repartition(n)``` - A new RDD with n partitions. This function uses a shuffle to redistribute data.\n",
    "  - ```.coalesce(n)``` - A new reduced RDD with n partitions. this function uses a shuffle to reduce number of partitions data performing better than ```.repartition()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repartition: 1\n",
      "After repartition: 2\n"
     ]
    }
   ],
   "source": [
    "rdd5 = sc.parallelize([1,3,12,42,55,5,6,8,34,54,7,9,93,5], 1)\n",
    "print(\"Before repartition:\", rdd5.getNumPartitions())\n",
    "rdd5 = rdd5.repartition(2)\n",
    "print(\"After repartition:\", rdd5.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before coalesce: 4\n",
      "After coalesce: 1\n"
     ]
    }
   ],
   "source": [
    "rdd6 = sc.parallelize([1,3,12,42,55,5,6,8,34,54,7,9,93,5], 4)\n",
    "print(\"Before coalesce:\", rdd6.getNumPartitions())\n",
    "rdd6 = rdd6.coalesce(1)\n",
    "print(\"After coalesce:\", rdd6.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save my RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```.saveAsTextFile()``` to save your RDD.\n",
    "\n",
    "PS: This function didn't work in jupyter notebook, I tried in jupyter lab and ipython and it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"sample_data/202-rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('a', 3)\", \"('b', 6)\", \"('a', 6)\", \"('c', 9)\", \"('a', 12)\", \"('c', 15)\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_temp = sc.textFile(\"sample_data/202-rdd.txt\")\n",
    "rdd_temp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to save as a hadoop file with the follow function:\n",
    "\n",
    "```.saveAsHadoopFile(\"path\", \"org.apache.hadoop.mapred.TextOutputFormat\")```\n",
    "\n",
    "Finishing this notebook with ```sc.stop()``` to shut down the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
